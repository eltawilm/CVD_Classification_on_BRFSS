{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbc8bfe-3755-4207-bd44-07248def9f9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T19:41:01.270766Z",
     "iopub.status.busy": "2025-09-20T19:41:01.265148Z",
     "iopub.status.idle": "2025-09-20T19:41:01.299814Z",
     "shell.execute_reply": "2025-09-20T19:41:01.297628Z",
     "shell.execute_reply.started": "2025-09-20T19:41:01.270766Z"
    }
   },
   "source": [
    "Verify whether oversampling-before-split inflates performance on CDC's BRFSS dataset.\n",
    "Replicating : https://pmc.ncbi.nlm.nih.gov/articles/PMC11678659/\n",
    "\n",
    "The script runs two experiments using BRFSS (2021):\n",
    "\n",
    "A) \"LEAKY\": SMOTE-ENN is applied to the dataset BEFORE the train/test split.\n",
    "\n",
    "B) \"PROPER\": Split first; oversampling (SMOTE-ENN) happens ONLY inside the training pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57359c04-b591-422e-a568-c31c3e7c017e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T12:31:49.704039Z",
     "iopub.status.busy": "2025-10-22T12:31:49.702877Z",
     "iopub.status.idle": "2025-10-22T12:31:49.711023Z",
     "shell.execute_reply": "2025-10-22T12:31:49.711023Z",
     "shell.execute_reply.started": "2025-10-22T12:31:49.704039Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.cdc.gov/brfss/annual_data/annual_data.htm\n",
    "# datafile downloaded from: https://www.cdc.gov/brfss/annual_data/2021/files/LLCP2021XPT.zip\n",
    "\n",
    "XPT_PATH = r\"C:\\Users\\meltawil\\Rutgers University\\DOPPS Data Kidney Project - General\\6.0 Team Working Folders\\Mohamed\\NHANES Files\"\n",
    "# XPT_PATH = \"/mnt/c/Users/meltawil/Rutgers University/DOPPS Data Kidney Project - General/6.0 Team Working Folders/Mohamed/NHANES Files/LLCP2021.XPT\"\n",
    "\n",
    "random_state = 42\n",
    "TEST_SIZE = 0.30\n",
    "SUBSAMPLE_FRAC = 0.1   # 20% used in the experiment to speed up processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924d09f-2d7f-45ca-b1d8-cf546f2803bf",
   "metadata": {},
   "source": [
    "### 1) IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f67d6ec8-e552-4def-92b3-cec2a180e49a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T12:31:49.713024Z",
     "iopub.status.busy": "2025-10-22T12:31:49.713024Z",
     "iopub.status.idle": "2025-10-22T12:31:52.822559Z",
     "shell.execute_reply": "2025-10-22T12:31:52.821335Z",
     "shell.execute_reply.started": "2025-10-22T12:31:49.713024Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7dbb9-39a9-44cf-aa40-6062fa763225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677ad457-0103-4ce2-921a-b3dd60acb860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-21T20:27:10.503150Z",
     "iopub.status.busy": "2025-09-21T20:27:10.502146Z",
     "iopub.status.idle": "2025-09-21T20:27:10.522848Z",
     "shell.execute_reply": "2025-09-21T20:27:10.521855Z",
     "shell.execute_reply.started": "2025-09-21T20:27:10.503150Z"
    }
   },
   "source": [
    "### 2) HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32c181c-fa0b-4508-be3d-d8ec51956dee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T12:31:52.823560Z",
     "iopub.status.busy": "2025-10-22T12:31:52.823560Z",
     "iopub.status.idle": "2025-10-22T12:31:52.852570Z",
     "shell.execute_reply": "2025-10-22T12:31:52.852570Z",
     "shell.execute_reply.started": "2025-10-22T12:31:52.823560Z"
    }
   },
   "outputs": [],
   "source": [
    "def recode_alcday5(series):\n",
    "    \"\"\"Recode ALCDAY5 to approximate drinks per month.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    out = pd.Series(np.nan, index=s.index, dtype=\"float\")\n",
    "\n",
    "    # Days per week (101–107)\n",
    "    mask_week = s.between(101, 107)\n",
    "    out[mask_week] = (s[mask_week] - 100) * 4.345  # weeks → ~days/month\n",
    "\n",
    "    # Days in past 30 days (201–230)\n",
    "    mask_month = s.between(201, 230)\n",
    "    out[mask_month] = s[mask_month] - 200\n",
    "\n",
    "    # No drinks in past 30 days\n",
    "    out[s == 888] = 0\n",
    "\n",
    "    # 777, 999, NaN → leave as NaN\n",
    "    return out\n",
    "\n",
    "\n",
    "def recode_food_frequency(series):\n",
    "    \"\"\"Recode FRUIT2, FVGREEN1, FRENCHF1 to times per month.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    out = pd.Series(np.nan, index=s.index, dtype=\"float\")\n",
    "\n",
    "    # 101–199: per day → ×30\n",
    "    mask = (s >= 101) & (s <= 199)\n",
    "    out[mask] = (s[mask] - 100) * 30\n",
    "\n",
    "    # 201–299: per week → ×4.3\n",
    "    mask = (s >= 201) & (s <= 299)\n",
    "    out[mask] = (s[mask] - 200) * 4.3\n",
    "\n",
    "    # 300: less than once per month\n",
    "    out[s == 300] = 0.5\n",
    "\n",
    "    # 301–399: per month\n",
    "    mask = (s >= 301) & (s <= 399)\n",
    "    out[mask] = s[mask] - 300\n",
    "\n",
    "    # 555: Never\n",
    "    out[s == 555] = 0\n",
    "\n",
    "    # 777, 999 → NaN\n",
    "    out[s.isin([777, 999])] = np.nan\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def prepare_brfss_dataset(df):\n",
    "    \"\"\"\n",
    "    Prepare BRFSS dataset for ML using the 18-paper-variable setup.\n",
    "    Works for BRFSS 2021, 2022, 2023 (if vars exist).\n",
    "    Returns: X (features), y (target).\n",
    "    \"\"\"\n",
    "\n",
    "    # Target: Heart disease (_MICHD)\n",
    "    y = df[\"_MICHD\"].replace({1: 1,2: 0, 7: np.nan, 9: np.nan})\n",
    "    y.name = \"Heart_Disease\"\n",
    "\n",
    "    # Features\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    X[\"General_Health\"] = df[\"GENHLTH\"].replace({1: 1, 2: 2, 3: 3, 4: 4, 5: 5,7: np.nan, 9: np.nan})\n",
    "    X[\"Checkup\"] = df[\"CHECKUP1\"].replace({1: 4, 2: 3, 3: 2, 4: 1,7: np.nan, 8: 0, 9: np.nan})\n",
    "    X[\"Exercise\"] = df[\"EXERANY2\"].replace({1: 1, 2: 0, 7: np.nan, 9: np.nan})\n",
    "    X[\"Skin_Cancer\"] = df[\"CHCSCNCR\"].replace({1: 1, 2: 0, 7: np.nan, 9: np.nan})\n",
    "    X[\"Other_Cancer\"] = df[\"CHCOCNCR\"].replace({1: 1, 2: 0, 7: np.nan, 9: np.nan})\n",
    "    X[\"Depression\"] = df[\"ADDEPEV3\"].replace({1: 1, 2: 0, 7: np.nan, 9: np.nan})\n",
    "    X[\"Diabetes\"] = df[\"DIABETE4\"].replace({1: 1, 2: 0, 3: 0, 4: 0, 7: np.nan, 9: np.nan})\n",
    "    X[\"Arthritis\"] = df[\"HAVARTH5\"].replace({1: 1, 2: 0, 7: np.nan, 9: np.nan})\n",
    "    X[\"Sex\"] = df[\"SEXVAR\"]\n",
    "    X[\"Age_Category\"] = df[\"_AGEG5YR\"].replace({\n",
    "        1: 12, 2: 11, 3: 10, 4: 9, 5: 8, 6: 7,\n",
    "        7: 6, 8: 5, 9: 4, 10: 3, 11: 2, 12: 1, 13: 0,\n",
    "        14: np.nan\n",
    "    })\n",
    "    X[\"Height_cm\"] = pd.to_numeric(df.get(\"HTM4\"), errors=\"coerce\")\n",
    "    X[\"Weight_kg\"] = pd.to_numeric(df.get(\"WTKG3\"), errors=\"coerce\") * 0.01\n",
    "    X[\"BMI\"] = X[\"Weight_kg\"] / ((X[\"Height_cm\"] * 0.01) ** 2)\n",
    "    X[\"Smoking_History\"] = df[\"SMOKE100\"].replace({1: 1, 2: 0, 7: np.nan, 9: np.nan})\n",
    "    X[\"Alcohol_Consumption\"] = recode_alcday5(df[\"ALCDAY5\"])\n",
    "    X[\"Fruit_Consumption\"] = recode_food_frequency(df[\"FRUIT2\"])\n",
    "    X[\"Green_Vegetables_Consumption\"] = recode_food_frequency(df[\"FVGREEN1\"])\n",
    "    X[\"FriedPotato_Consumption\"] = recode_food_frequency(df[\"FRENCHF1\"])\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db1a0640-2a77-46d2-8159-277030e6926c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T12:31:52.854559Z",
     "iopub.status.busy": "2025-10-22T12:31:52.854559Z",
     "iopub.status.idle": "2025-10-22T12:31:52.863124Z",
     "shell.execute_reply": "2025-10-22T12:31:52.862344Z",
     "shell.execute_reply.started": "2025-10-22T12:31:52.854559Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_brfss_dataset(X, y, drop_na=True, drop_dupes=True,\n",
    "                        height_range=(140, 210), weight_range=(45, 200),\n",
    "                        verbose=True):\n",
    "    \"\"\"\n",
    "    Clean BRFSS dataset after feature/target preparation.\n",
    "    - Drops duplicate rows\n",
    "    - Optionally drops rows with NaNs\n",
    "    - Applies height/weight plausibility filters\n",
    "    - Returns cleaned (X, y)\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine into single DataFrame\n",
    "    df_xy = pd.concat([X, y], axis=1)\n",
    "\n",
    "    n_before = len(df_xy)\n",
    "\n",
    "    if drop_dupes:\n",
    "        df_xy = df_xy.drop_duplicates()\n",
    "    n_after_dupes = len(df_xy)\n",
    "\n",
    "    if drop_na:\n",
    "        df_xy = df_xy.dropna()\n",
    "    n_after_na = len(df_xy)\n",
    "\n",
    "    # Apply range filters\n",
    "    mask = (\n",
    "        df_xy[\"Height_cm\"].between(*height_range) &\n",
    "        df_xy[\"Weight_kg\"].between(*weight_range)\n",
    "    )\n",
    "    df_xy = df_xy[mask]\n",
    "    n_after_range = len(df_xy)\n",
    "\n",
    "    # Split back\n",
    "    X_clean = df_xy.drop(columns=[\"Heart_Disease\"])\n",
    "    y_clean = df_xy[\"Heart_Disease\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Initial rows: {n_before:,}\")\n",
    "        if drop_dupes:\n",
    "            print(f\"After dropping duplicates: {n_after_dupes:,} (removed {n_before - n_after_dupes:,})\")\n",
    "        if drop_na:\n",
    "            print(f\"After dropping NaNs: {n_after_na:,} (removed {n_after_dupes - n_after_na:,})\")\n",
    "        print(f\"After filtering Height[{height_range[0]}–{height_range[1]}] & \"\n",
    "              f\"Weight[{weight_range[0]}–{weight_range[1]}]: {n_after_range:,} \"\n",
    "              f\"(removed {n_after_na - n_after_range:,})\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return X_clean, y_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403397ea-e9fd-467e-8274-b4d2a351e6d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T12:31:52.865675Z",
     "iopub.status.busy": "2025-10-22T12:31:52.864751Z",
     "iopub.status.idle": "2025-10-22T12:31:52.879207Z",
     "shell.execute_reply": "2025-10-22T12:31:52.877997Z",
     "shell.execute_reply.started": "2025-10-22T12:31:52.865675Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_brfss_dataset(X, y, test_size=TEST_SIZE, subsample_frac=SUBSAMPLE_FRAC,\n",
    "                        random_state=random_state, verbose=True):\n",
    "    \"\"\"\n",
    "    Prepares BRFSS dataset for ML:\n",
    "    - Drops rows with missing labels (NaN in y)\n",
    "    - Optionally subsamples while preserving class balance\n",
    "    - Splits into train/test sets with stratification\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop NaN labels\n",
    "    mask = y.notna()\n",
    "    X = X.loc[mask].reset_index(drop=True)\n",
    "    y = y.loc[mask].astype(int).reset_index(drop=True)\n",
    "\n",
    "    # # Subsample\n",
    "    # if subsample_frac < 1.0:\n",
    "    #     X, _, y, _ = train_test_split(\n",
    "    #         X, y,\n",
    "    #         test_size=(1 - subsample_frac),\n",
    "    #         random_state=random_state,\n",
    "    #         stratify=y\n",
    "    #     )\n",
    "    # else:\n",
    "    #     X, y = X.copy(), y.copy()\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(f\"Subsampled dataset: {len(X):,} rows, positives={y.sum():,} ({y.mean()*100:.2f}%)\\n\")\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        def summarize_balance(name, y_arr):\n",
    "            p = float((y_arr == 1).mean())\n",
    "            n = len(y_arr)\n",
    "            print(f\"{name:>10s}  n={n:>8d}  positives={p*100:5.2f}%  (class balance)\")\n",
    "        print(\"\\n=== Class balance (original and splits) ===\")\n",
    "        summarize_balance(\"ALL\", y)\n",
    "        summarize_balance(\"TRAIN\", y_train)\n",
    "        summarize_balance(\"TEST\", y_test)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca2cf9d-273f-4379-bc81-a16910becac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T12:31:52.879207Z",
     "iopub.status.busy": "2025-10-22T12:31:52.879207Z",
     "iopub.status.idle": "2025-10-22T12:31:52.893649Z",
     "shell.execute_reply": "2025-10-22T12:31:52.892910Z",
     "shell.execute_reply.started": "2025-10-22T12:31:52.879207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper transformer to cast to float32\n",
    "# to_float32 = FunctionTransformer(lambda x: x.astype(np.float32))\n",
    "\n",
    "def make_knn_pipeline(leaky=False):\n",
    "    KNN_args = {\"n_neighbors\":2,\n",
    "                \"metric\":\"euclidean\",\n",
    "                \"weights\":\"uniform\",\n",
    "                \"algorithm\":\"brute\"}\n",
    "    \n",
    "    if leaky:\n",
    "        return ImbPipeline([\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"to32\", FunctionTransformer(lambda x: x.astype(np.float32))),\n",
    "            (\"clf\", KNeighborsClassifier(**KNN_args)),        \n",
    "            ])\n",
    "    else:\n",
    "        return ImbPipeline([\n",
    "            (\"scaler\", MinMaxScaler()),\n",
    "            (\"smoteenn\", smoteenn),                         \n",
    "            (\"to32\", FunctionTransformer(lambda x: x.astype(np.float32))),\n",
    "            (\"clf\", KNeighborsClassifier(**KNN_args)),        \n",
    "            ])\n",
    "\n",
    "\n",
    "# def evaluate(pipe, Xval, yval, label):\n",
    "#     preds = pipe.predict(Xval)\n",
    "\n",
    "#     try:\n",
    "#         proba = pipe.predict_proba(Xval)[:, 1]\n",
    "#         auc = roc_auc_score(yval, proba)\n",
    "#     except Exception:\n",
    "#         proba, auc = None, np.nan\n",
    "\n",
    "#     acc = accuracy_score(yval, preds)\n",
    "#     f1  = f1_score(yval, preds)\n",
    "#     cm  = confusion_matrix(yval, preds)\n",
    "#     cr  = classification_report(yval, preds, digits=3, output_dict=True)  # structured dict\n",
    "\n",
    "#     # Print summary\n",
    "#     print(f\"\\n[{label}]\")\n",
    "#     print(f\"ACC: {acc:.4f}  F1: {f1:.4f}  AUC: {auc:.4f}\")\n",
    "#     print(\"Confusion matrix:\\n\", cm)\n",
    "#     print(\"Classification report:\\n\", classification_report(yval, preds, digits=3))\n",
    "\n",
    "#     # Return as dictionary\n",
    "#     return {\n",
    "#         \"label\": label,\n",
    "#         \"accuracy\": acc,\n",
    "#         \"f1\": f1,\n",
    "#         \"auc\": auc,\n",
    "#         \"confusion_matrix\": cm,\n",
    "#         \"classification_report\": cr\n",
    "#     }\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def evaluate(pipe, Xval, yval, label):\n",
    "    preds = pipe.predict(Xval)\n",
    "\n",
    "    # Ensure y is 0/1 binary\n",
    "    y_bin = np.array(yval).ravel()\n",
    "\n",
    "    proba, auc, fpr, tpr = None, np.nan, [], []\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        try:\n",
    "            raw_proba = pipe.predict_proba(Xval)\n",
    "            if raw_proba.ndim == 2 and raw_proba.shape[1] > 1:\n",
    "                proba = raw_proba[:, 1]\n",
    "            else:\n",
    "                proba = raw_proba.ravel()\n",
    "\n",
    "            auc = roc_auc_score(y_bin, proba)\n",
    "            fpr, tpr, _ = roc_curve(y_bin, proba)\n",
    "\n",
    "            print(f\"[{label}] ROC computed: {len(fpr)} points\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{label}] ROC failed: {e}\")\n",
    "\n",
    "    acc = accuracy_score(yval, preds)\n",
    "    f1  = f1_score(yval, preds)\n",
    "    cm  = confusion_matrix(yval, preds)\n",
    "    cr  = classification_report(yval, preds, digits=3, output_dict=True)\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": cr,\n",
    "        \"fpr\": fpr,   # always return (even if empty)\n",
    "        \"tpr\": tpr\n",
    "    }\n",
    "\n",
    "# def evaluate(pipe, Xval, yval, label):\n",
    "#     preds = pipe.predict(Xval)\n",
    "\n",
    "#     # Force numpy array\n",
    "#     y_bin = np.array(yval).ravel()\n",
    "\n",
    "#     proba, auc, fpr, tpr = None, np.nan, [], []\n",
    "#     if hasattr(pipe, \"predict_proba\"):\n",
    "#         try:\n",
    "#             raw_proba = pipe.predict_proba(Xval)\n",
    "#             print(f\"[{label}] predict_proba shape: {raw_proba.shape}\")\n",
    "\n",
    "#             # Handle binary output\n",
    "#             if raw_proba.ndim == 2 and raw_proba.shape[1] > 1:\n",
    "#                 proba = raw_proba[:, 1]\n",
    "#             else:\n",
    "#                 proba = raw_proba.ravel()\n",
    "\n",
    "#             print(f\"[{label}] proba sample: {proba[:10]}\")  # first 10 probs\n",
    "\n",
    "#             auc = roc_auc_score(y_bin, proba)\n",
    "#             fpr, tpr, _ = roc_curve(y_bin, proba)\n",
    "\n",
    "#             print(f\"[{label}] ROC lengths: fpr={len(fpr)}, tpr={len(tpr)}\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"[{label}] ROC failed: {e}\")\n",
    "\n",
    "#     acc = accuracy_score(yval, preds)\n",
    "#     f1  = f1_score(yval, preds)\n",
    "#     cm  = confusion_matrix(yval, preds)\n",
    "#     cr  = classification_report(yval, preds, digits=3, output_dict=True)\n",
    "\n",
    "#     return {\n",
    "#         \"label\": label,\n",
    "#         \"accuracy\": acc,\n",
    "#         \"f1\": f1,\n",
    "#         \"auc\": auc,\n",
    "#         \"confusion_matrix\": cm,\n",
    "#         \"classification_report\": cr,\n",
    "#         \"fpr\": fpr,\n",
    "#         \"tpr\": tpr\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e28e2da-ddc3-44ca-b46d-1d38de74614f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T12:31:52.895947Z",
     "iopub.status.busy": "2025-10-22T12:31:52.894885Z",
     "iopub.status.idle": "2025-10-22T12:31:59.772653Z",
     "shell.execute_reply": "2025-10-22T12:31:59.770652Z",
     "shell.execute_reply.started": "2025-10-22T12:31:52.894885Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'kullback_leibler_divergence' from 'keras.losses' (C:\\Users\\meltawil\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\losses\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# NEW: use SciKeras wrapper instead of the removed TF one\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, BatchNormalization\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\scikeras\\__init__.py:9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mimportlib_metadata\u001b[39;00m\n\u001b[0;32m      7\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m importlib_metadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikeras\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_keras\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _saving_utils\n\u001b[0;32m     13\u001b[0m _keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39m__reduce__ \u001b[38;5;241m=\u001b[39m _saving_utils\u001b[38;5;241m.\u001b[39mpack_keras_model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\engine\\functional.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_spec\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_utils\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m json_utils\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\engine\\training.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer_utils\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compile_utils\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_adapter\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_layer \u001b[38;5;28;01mas\u001b[39;00m input_layer_module\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\engine\\compile_utils.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses \u001b[38;5;28;01mas\u001b[39;00m losses_mod\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics \u001b[38;5;28;01mas\u001b[39;00m metrics_mod\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generic_utils\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\metrics\\__init__.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_metric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone_metrics\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Metric functions\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Individual metric classes\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUC\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accuracy\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryAccuracy\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\metrics\\metrics.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m categorical_hinge\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hinge\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m kullback_leibler_divergence\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logcosh\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlosses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'kullback_leibler_divergence' from 'keras.losses' (C:\\Users\\meltawil\\anaconda3\\envs\\gpu-env\\lib\\site-packages\\keras\\losses\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# NEW: use SciKeras wrapper instead of the removed TF one\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "\n",
    "\n",
    "def build_ann(input_dim=None):\n",
    "    \"\"\"ANN architecture per paper (binary classification).\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def make_pipeline(model_name=\"knn\", leaky=False, smoteenn=None):\n",
    "    \"\"\"\n",
    "    Build an imbalanced pipeline with classifier chosen by `model_name`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        One of: 'knn', 'rf', 'lr', 'nb', 'svc', 'xgb', 'ann'\n",
    "    leaky : bool\n",
    "        If True, skip SMOTE–ENN step.\n",
    "    smoteenn : transformer\n",
    "        SMOTE-ENN object (or any sampler) to insert if not leaky.\n",
    "    \"\"\"\n",
    "\n",
    "    clf_dict = {\n",
    "        \"knn\": KNeighborsClassifier(\n",
    "            n_neighbors=2, metric=\"euclidean\", weights=\"uniform\", algorithm=\"brute\"\n",
    "        ),\n",
    "        \"rf\": RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None,\n",
    "            min_samples_split=2, min_samples_leaf=1, random_state=42\n",
    "        ),\n",
    "        \"lr\": LogisticRegression(\n",
    "            solver=\"saga\", penalty=\"l1\", max_iter=500, C=0.08858667904100823\n",
    "        ),\n",
    "        \"nb\": GaussianNB(var_smoothing=1e-7, priors=[0.3, 0.7]),\n",
    "        \"svc\": SVC(kernel=\"rbf\", probability=True, random_state=42),\n",
    "        \"xgb\": XGBClassifier(\n",
    "            colsample_bytree=1.0, learning_rate=0.2, max_depth=7,\n",
    "            n_estimators=300, subsample=0.9,\n",
    "            reg_alpha=0.5, reg_lambda=0.5, use_label_encoder=False,\n",
    "            eval_metric=\"logloss\", random_state=42\n",
    "        ),\n",
    "        \"ann\": KerasClassifier(\n",
    "            model=build_ann,\n",
    "            model__input_dim=X.shape[1],  # <-- tell SciKeras how many features\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        ),        \n",
    "    }\n",
    "\n",
    "    if model_name not in clf_dict:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "    steps = [(\"scaler\", MinMaxScaler())]\n",
    "\n",
    "    if not leaky and smoteenn is not None:\n",
    "        steps.append((\"smoteenn\", smoteenn))\n",
    "\n",
    "    steps.extend([\n",
    "        (\"to32\", FunctionTransformer(lambda x: x.astype(np.float32))),\n",
    "        (\"clf\", clf_dict[model_name])\n",
    "    ])\n",
    "\n",
    "    return ImbPipeline(steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f95c7f-c1ac-4ded-b0e5-0d10b97c3a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fca5143-f2a8-46c4-af3f-ad7814ead41c",
   "metadata": {},
   "source": [
    "### 3) LOAD & PREPROCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79db4316-e5ef-485c-b8fb-ed7614eb4c6d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.772653Z",
     "iopub.status.idle": "2025-10-22T12:31:59.773843Z",
     "shell.execute_reply": "2025-10-22T12:31:59.772653Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.772653Z"
    }
   },
   "outputs": [],
   "source": [
    "# ===== LOAD  ===== \n",
    "df = pd.read_sas(os.path.join(XPT_PATH, \"LLCP2021.XPT\"), format=\"xport\", encoding=\"utf-8\")\n",
    "df.columns = [str(c).upper() for c in df.columns]\n",
    "print(f\"Rows: {len(df):,}, Columns: {len(df.columns):,}\\n\")\n",
    "\n",
    "# ===== MAP FIELDS  ===== \n",
    "X, y = prepare_brfss_dataset(df)\n",
    "\n",
    "# ===== Target distribution =====\n",
    "counts = y.value_counts(dropna=False)\n",
    "percentages = (counts / len(y) * 100).round(2)\n",
    "# print(\"\\nTarget distribution (Heart_Disease):\")\n",
    "print(pd.DataFrame({\"Count\": counts, \"Percent\": percentages}))\n",
    "print(\"\\n\")\n",
    "\n",
    "# ===== DATA CLEANING  ===== \n",
    "X_all, y_all = clean_brfss_dataset(X, y)\n",
    "\n",
    "# ===== SUBSAMPLE  ===== \n",
    "if SUBSAMPLE_FRAC < 1.0:\n",
    "    X, _, y, _ = train_test_split(X_all, y_all,test_size=(1 - SUBSAMPLE_FRAC),random_state=random_state,stratify=y_all)\n",
    "    print(f\"Subsampled dataset: {len(X):,} rows, positives={y.sum():,} ({y.mean()*100:.2f}%)\\n\")\n",
    "else:\n",
    "    X, y = X.copy(), y.copy()\n",
    "\n",
    "# ===== DATA SPLITTING  ===== \n",
    "X_train, X_test, y_train, y_test = split_brfss_dataset(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3068cb76-1f6b-4358-8bdb-4d853d0deb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9a6b1-96a9-487b-b53b-9ff387c54400",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.773843Z",
     "iopub.status.idle": "2025-10-22T12:31:59.774844Z",
     "shell.execute_reply": "2025-10-22T12:31:59.773843Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.773843Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Combine features + target\n",
    "# df_corr = X.copy()\n",
    "# df_corr[\"Heart_Disease\"] = y\n",
    "\n",
    "# # Compute correlations (numeric only)\n",
    "# corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# # Plot heatmap with annotations\n",
    "# plt.figure(figsize=(14, 10))\n",
    "# sns.heatmap(corr,cmap=\"coolwarm\",annot=True,fmt=\".2f\",center=0,cbar_kws={\"shrink\": 0.75})\n",
    "# plt.title(\"Correlation Heatmap of Features and Heart_Disease\", fontsize=16, pad=20)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26325b9-bffa-4b77-9706-69f1f958ad81",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.774844Z",
     "iopub.status.idle": "2025-10-22T12:31:59.774844Z",
     "shell.execute_reply": "2025-10-22T12:31:59.774844Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.774844Z"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79a004-20aa-42fd-9472-0e9d567790a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "742ce0cc-aeeb-41b2-9beb-52abb0657f33",
   "metadata": {},
   "source": [
    "### 4) EXPERIMENT A — **LEAKY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5297d4a-553e-4b8e-8ac5-b4f61d890719",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.775844Z",
     "iopub.status.idle": "2025-10-22T12:31:59.775844Z",
     "shell.execute_reply": "2025-10-22T12:31:59.775844Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.775844Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# EXPERIMENT A: LEAKY (SMOTE-ENN applied BEFORE split)\n",
    "# ======================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT A: LEAKY (SMOTE-ENN applied BEFORE split)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "smoteenn = SMOTEENN(random_state=random_state, sampling_strategy=\"auto\", n_jobs=-1)\n",
    "\n",
    "# 1) Apply SMOTE-ENN globally (this is the 'leaky' mistake)\n",
    "X_leaky, y_leaky = smoteenn.fit_resample(X, y)\n",
    "print(f\"After SMOTE-ENN (global): rows={len(X_leaky):,}, \"\n",
    "      f\"positives={(y_leaky==1).sum():,}, negatives={(y_leaky==0).sum():,}\")\n",
    "\n",
    "# 2) Split AFTER resampling (leakage!) & run pipeline\n",
    "Xtr_L, Xte_L, ytr_L, yte_L = train_test_split(X_leaky, y_leaky,test_size=TEST_SIZE,random_state=random_state,stratify=y_leaky)\n",
    "\n",
    "# 3) Run ALL models\n",
    "model_names = [\"knn\", \"rf\", \"lr\", \"nb\", \"xgb\"] #, \"svc\", \"ann\"]\n",
    "results_leaky = {}\n",
    "\n",
    "for name in model_names:\n",
    "    print(f\"\\n--- Training {name.upper()} ---\")\n",
    "    start = time.time()\n",
    "    pipe = make_pipeline(model_name=name, leaky=True)\n",
    "    pipe.fit(Xtr_L, ytr_L)\n",
    "    results_leaky[name] = evaluate(pipe, Xte_L, yte_L, f\"LEAKY {name.upper()}\")\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"Time taken for {name.upper()}: {elapsed:.2f} seconds\\n\\n\")\n",
    "\n",
    "print(\"\\nCompleted Experiment A (Leaky).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d03649-406b-4a6b-8764-cf299b884a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83586c10-53bd-4499-b962-3b70a56c7ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4270998-258a-4ee6-95b9-acf6d5382ccd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.776776Z",
     "iopub.status.idle": "2025-10-22T12:31:59.776776Z",
     "shell.execute_reply": "2025-10-22T12:31:59.776776Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.776776Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_comparison_table(results_dict):\n",
    "    \"\"\"\n",
    "    Build a comparison DataFrame from evaluate() outputs.\n",
    "    results_dict: dict of {model_name: evaluate_output}\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for name, res in results_dict.items():\n",
    "        cr = res[\"classification_report\"]\n",
    "\n",
    "        # Find the positive class key (exclude 'accuracy', 'macro avg', 'weighted avg')\n",
    "        class_keys = [k for k in cr.keys() if k not in [\"accuracy\", \"macro avg\", \"weighted avg\"]]\n",
    "        if len(class_keys) == 2:\n",
    "            # assume binary: take the positive class as the *larger* label\n",
    "            pos_class = sorted(class_keys)[-1]\n",
    "        else:\n",
    "            # fallback: just take the last class\n",
    "            pos_class = class_keys[-1]\n",
    "\n",
    "        rows.append({\n",
    "            \"Method\": name.upper(),\n",
    "            \"AUC\": res[\"auc\"],\n",
    "            \"ACC\": res[\"accuracy\"],\n",
    "            \"F1\": res[\"f1\"],\n",
    "            \"Precision\": cr[pos_class][\"precision\"],\n",
    "            \"Recall\": cr[pos_class][\"recall\"],\n",
    "            \"Specificity\": res[\"confusion_matrix\"][0,0] / res[\"confusion_matrix\"][0].sum()\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.set_index(\"Method\")\n",
    "\n",
    "\n",
    "def plot_all_rocs(results_dict, title=\"ROC Curves Comparison\"):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for name, res in results_dict.items():\n",
    "        fpr, tpr, auc = res.get(\"fpr\"), res.get(\"tpr\"), res.get(\"auc\")\n",
    "        if fpr is not None and tpr is not None and len(fpr) > 0:\n",
    "            plt.plot(fpr, tpr, label=f\"{name.upper()} (AUC={auc:.4f})\")\n",
    "        else:\n",
    "            print(f\"[{name.upper()}] No ROC curve data available.\")\n",
    "    plt.plot([0,1],[0,1],\"k--\",label=\"Chance\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6c4d9-e3c6-4ab5-9147-e3fcfbd30171",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.777797Z",
     "iopub.status.idle": "2025-10-22T12:31:59.777797Z",
     "shell.execute_reply": "2025-10-22T12:31:59.777797Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.777797Z"
    }
   },
   "outputs": [],
   "source": [
    "df_leaky  = build_comparison_table(results_leaky)\n",
    "\n",
    "print(\"\\n=== LEAKY Results ===\")\n",
    "print(df_leaky.round(4))\n",
    "\n",
    "plot_all_rocs(results_leaky, title=\"Experiment A: Leaky ROC Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19cf8c6-227d-499f-a9d0-86376cc4de2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e028ff-d464-4586-a97e-de382f8c6829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbcd2229-f096-4b87-bdbb-b9bd30839b33",
   "metadata": {},
   "source": [
    "### 5) EXPERIMENT B — **PROPER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5db6f-df8c-4d1f-b3a3-9a5f824e399e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.777797Z",
     "iopub.status.idle": "2025-10-22T12:31:59.780227Z",
     "shell.execute_reply": "2025-10-22T12:31:59.777797Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.777797Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# EXPERIMENT B: PROPER (split first; resample only on the training folds)\n",
    "# ======================================================================\n",
    "\n",
    "# Split first; then run SMOTE-ENN ONLY within training via imblearn Pipeline.\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT B: PROPER (split first; resample only on the training folds)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Run ALL models\n",
    "results_proper1 = {}\n",
    "\n",
    "for name in model_names:\n",
    "    print(f\"\\n--- Training {name.upper()} ---\")\n",
    "    pipe = make_pipeline(model_name=name, leaky=False, smoteenn=smoteenn)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    results_proper1[name] = evaluate(pipe, X_test, y_test, f\"PROPER {name.upper()}\")\n",
    "\n",
    "print(\"\\nCompleted Experiment B (Proper).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0fce44-9d09-4c90-bd2b-e78be39f2d78",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.781238Z",
     "iopub.status.idle": "2025-10-22T12:31:59.782237Z",
     "shell.execute_reply": "2025-10-22T12:31:59.782237Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.782237Z"
    }
   },
   "outputs": [],
   "source": [
    "df_leaky  = build_comparison_table(results_leaky)\n",
    "\n",
    "print(\"\\n=== PROPER (1) Results ===\")\n",
    "print(df_leaky.round(4))\n",
    "\n",
    "plot_all_rocs(results_proper1, title=\"Experiment A: Proper (1) ROC Curves\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca02a8d-dd43-44e2-83e5-b5bf6c2594d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-23T19:35:41.657291Z",
     "iopub.status.busy": "2025-09-23T19:35:41.657291Z",
     "iopub.status.idle": "2025-09-23T19:35:41.663600Z",
     "shell.execute_reply": "2025-09-23T19:35:41.663600Z",
     "shell.execute_reply.started": "2025-09-23T19:35:41.657291Z"
    }
   },
   "source": [
    "### 6) EXPERIMENT C — **PROPER** RandomUnderSampler applied BEFORE split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e5caaa-5166-4f8f-be1e-f6eaa0551bee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.783242Z",
     "iopub.status.idle": "2025-10-22T12:31:59.783242Z",
     "shell.execute_reply": "2025-10-22T12:31:59.783242Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.783242Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================\n",
    "# EXPERIMENT C: LEAKY (RandomUnderSampler applied BEFORE split)\n",
    "# ======================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT C: LEAKY (RandomUnderSampler applied BEFORE split)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rus = RandomUnderSampler(random_state=random_state, sampling_strategy=\"auto\")\n",
    "\n",
    "# 1) Apply undersampling globally (this is the 'leaky' mistake)\n",
    "X_leaky_us, y_leaky_us = rus.fit_resample(X_all, y_all)\n",
    "print(f\"After RandomUnderSampler (global): rows={len(X_leaky_us):,}, \"\n",
    "      f\"positives={(y_leaky_us==1).sum():,}, negatives={(y_leaky_us==0).sum():,}\")\n",
    "\n",
    "# 2) Split AFTER resampling (leakage!)\n",
    "Xtr_L_us, Xte_L_us, ytr_L_us, yte_L_us = train_test_split(\n",
    "    X_leaky_us, y_leaky_us,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=random_state,\n",
    "    stratify=y_leaky_us\n",
    ")\n",
    "\n",
    "# 3) Run ALL models\n",
    "model_names = [\"knn\", \"rf\", \"lr\", \"nb\", \"xgb\"]  # add \"svc\", \"ann\" later if needed\n",
    "results_leaky_us = {}\n",
    "\n",
    "for name in model_names:\n",
    "    print(f\"\\n--- Training {name.upper()} (Undersample) ---\")\n",
    "    start = time.time()\n",
    "    pipe = make_pipeline(model_name=name, leaky=True)  # still \"leaky\" since resampling was global\n",
    "    pipe.fit(Xtr_L_us, ytr_L_us)\n",
    "    results_leaky_us[name] = evaluate(pipe, Xte_L_us, yte_L_us, f\"LEAKY-US {name.upper()}\")\n",
    "    end = time.time()\n",
    "    print(f\"Time taken for {name.upper()}: {end-start:.2f} seconds\\n\")\n",
    "\n",
    "print(\"\\nCompleted Experiment C (Leaky with Random Undersampling).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664276b-8f6b-4488-918d-bb6548db5ae7",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-22T12:31:59.784232Z",
     "iopub.status.idle": "2025-10-22T12:31:59.784232Z",
     "shell.execute_reply": "2025-10-22T12:31:59.784232Z",
     "shell.execute_reply.started": "2025-10-22T12:31:59.784232Z"
    }
   },
   "outputs": [],
   "source": [
    "df_leaky  = build_comparison_table(results_leaky)\n",
    "\n",
    "print(\"\\n=== LEAKY Results ===\")\n",
    "print(results_leaky_us.round(4))\n",
    "\n",
    "plot_all_rocs(results_leaky_us, title=\"Experiment A: Leaky ROC Curves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285dc05-d4f5-4484-8b93-93ddb0234d99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f16d7-2ad0-44fb-a07c-37b4292e362d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0cd5472-6054-44ce-ac35-aeca01458540",
   "metadata": {},
   "source": [
    "### 6) COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a6816-0228-416e-bfab-ac8d681a93a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055529a2-7562-4680-99ff-0540caa0ea93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37095af2-6fdb-42c8-a012-cb9a02788dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b74d8-3f08-4ea1-8d5b-b9d26600d966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpu-env)",
   "language": "python",
   "name": "gpu-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
